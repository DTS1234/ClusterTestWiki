{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Cluster Test Wiki This documentation provides information and guidance for the Cluster Probe and Probe Builder projects. These software packages are designed to facilitate testing of Kubernetes clusters in terms of scalability and performance. The primary objective of these projects is to create a software-based solution for testing typical cloud service use cases within the context of reCluster project, a self-aware cloud orchestration system. Through this documentation, you will gain insights into the functionalities, architecture, implementation, testing, API specifications, and user guide for both Cluster Probe and Probe Builder. Whether you are new to testing Kubernetes clusters or seeking to enhance your existing knowledge, this documentation will serve as a valuable resource to understand and utilize these tools effectively. Table of contents Overview Probe builder Cluster probe Testing Recluster","title":"Welcome to Cluster Test Wiki"},{"location":"#welcome-to-cluster-test-wiki","text":"This documentation provides information and guidance for the Cluster Probe and Probe Builder projects. These software packages are designed to facilitate testing of Kubernetes clusters in terms of scalability and performance. The primary objective of these projects is to create a software-based solution for testing typical cloud service use cases within the context of reCluster project, a self-aware cloud orchestration system. Through this documentation, you will gain insights into the functionalities, architecture, implementation, testing, API specifications, and user guide for both Cluster Probe and Probe Builder. Whether you are new to testing Kubernetes clusters or seeking to enhance your existing knowledge, this documentation will serve as a valuable resource to understand and utilize these tools effectively.","title":"Welcome to Cluster Test Wiki"},{"location":"#table-of-contents","text":"Overview Probe builder Cluster probe Testing Recluster","title":"Table of contents"},{"location":"cluster-probe/design/","text":"Design The design of Cluster Probe follows a straightforward approach where each feature is implemented as a separate service. These services can be injected as Spring beans into the Controller class, which acts as the REST API implementation. the use of dependency injection through Spring beans enhances code modularity and testability. By decoupling components and providing dependencies through injection, the design promotes flexibility and makes it easier to introduce changes or extend functionality without impacting other parts of the system. Error handling is handled by an ErrorHandler class, which returns the appropriate HTTP response entities with the correct status code and message. This class allows to provide a uniform approach to handling exceptions and generating meaningful error responses. This practice improves the overall readability of the tests that will allow to better analyze the errors. The design includes model classes, such as TestSpecification for stress-ng job specifications and FileOperationSpecification for file operations endpoints. For transactional database tests, there is a separate controller to manage these specific cases. Additionally, a class following the Repository pattern is responsible for saving and retrieving objects from the database. This provides a structured representation of the data being processed. This separation of data models helps to enforce data integrity and ensures consistent handling of inputs and outputs across the application. In summary, the Controller class receives requests along with the corresponding specification objects, which are then processed by the service layer to execute the required logic. Errors are handled by the ErrorHandler, and the transactional database tests have their own controller and a dedicated class for database operations.","title":"Design"},{"location":"cluster-probe/design/#design","text":"The design of Cluster Probe follows a straightforward approach where each feature is implemented as a separate service. These services can be injected as Spring beans into the Controller class, which acts as the REST API implementation. the use of dependency injection through Spring beans enhances code modularity and testability. By decoupling components and providing dependencies through injection, the design promotes flexibility and makes it easier to introduce changes or extend functionality without impacting other parts of the system. Error handling is handled by an ErrorHandler class, which returns the appropriate HTTP response entities with the correct status code and message. This class allows to provide a uniform approach to handling exceptions and generating meaningful error responses. This practice improves the overall readability of the tests that will allow to better analyze the errors. The design includes model classes, such as TestSpecification for stress-ng job specifications and FileOperationSpecification for file operations endpoints. For transactional database tests, there is a separate controller to manage these specific cases. Additionally, a class following the Repository pattern is responsible for saving and retrieving objects from the database. This provides a structured representation of the data being processed. This separation of data models helps to enforce data integrity and ensures consistent handling of inputs and outputs across the application. In summary, the Controller class receives requests along with the corresponding specification objects, which are then processed by the service layer to execute the required logic. Errors are handled by the ErrorHandler, and the transactional database tests have their own controller and a dedicated class for database operations.","title":"Design"},{"location":"cluster-probe/implementation/","text":"Implementation Introduction Cluster Probe is written in Java 17 , and it uses the Spring Boot Framework . Each feature's code is stored in its own java package. All packages expose a service class that can be injected as a dependency to the Controller, the REST api web layer class. Any error that has occurred during the test is handled by the ErrorHandler class that specifies an appropriate message and returns the error response entity. File operations FileOperations service is using java standard development kit libraries to implement the creation, update and the deletion for files. Those operations are performed based on the data specified in FileSystemSpecification class object. there is only one public method implemented for the FileOperationsService all the files created will be stored under the projects directory in the 'test' folder, then the number of files will be created based on parameters specified in the specification object. Specification class If the fileContent parameter will be null or empty no write operations will be performed. After all the operations are performed files and the 'test' directory are deleted: The FileService can be invoked from the Controller class by calling an HTTP POST method. @PostMapping(\"/api/file-operations\") public String createFiles(@RequestBody FileSystemSpecification spec) { fileOperationsService.testFileSystem(spec); return String.format(\"File operations performed based on spec %s\", spec); } Here, createFiles method handles the POST request for the file-operations endpoint, accepts a FileSystemSpecification object in the request body, and delegates the file operations logic to the fileOperationsService . It responds with a message confirming the successful execution of the file operations based on the provided specification. Stress ng In order to implement the stress ng jobs invocation, TestService creates an appropriate stress ng job based on the TestSpecification parameters, not all parameters are required so mostly TestService class implements the logic for creating the job accurately. Specification class for each parameter in the specification there is an according mapping method in the service class that based on whether the parameter is correct or not it creates an appropriate part of the command. There is also an option for receiving a plain command as a string, if this one is available and the isCommand flag is set to true the rest of the parameters will be ignored. Whenever the durationInSeconds parameter will not be available there is a default 24 hours value set in the mapTestTimeCommand : The TestService can be invoked from the Controller class by calling an HTTP POST method. @PostMapping(\"/api/job\") public String startJob(@RequestBody TestSpecification specification) { log.info(\"Received job specification: \" + specification); boolean jobStarted = testService.startJobSpecification(specification); if (jobStarted) { log.info(\"Job started for specification: \" + specification); } else { log.error(\"Job failed to start for specification: \" + specification); } return \"Job started\"; } The method delegates the processing of the request to the testService by calling the startJobSpecification method and passing the specification object. Depending on the result returned by the service layer, the method logs appropriate messages. If the job is successfully started, it logs that the job has started. If the job fails to start, it logs an error message. The method returns a simple string message indicating that the job has started. Transactional In order to give the users an availability to test multiple database connections with transactional operations, a small example for the school domain have been created. It represents a classical model of a student - course many to many relationship. The Student class represents a student in the educational domain. It is annotated with @Entity to indicate that it is a JPA entity mapped to a database table. The class has the following attributes: id : A unique identifier for the student, annotated with @Id and @GeneratedValue to indicate that it is the primary key and its value is automatically generated. name : The name of the student, annotated with @Column to specify the mapping to the corresponding column in the database table. courses : A many-to-many relationship with the Course class, representing the courses that the student is enrolled in. The relationship is defined using the @ManyToMany annotation, and the mapping is managed through a join table named \"course_signed\". The Course class represents a course offered in the educational domain. It has similar annotations and attributes as the Student class: id : The unique identifier for the course. name : The name of the course. students : A bidirectional many-to-many relationship with the Student class. It is mapped by the courses attribute in the Student class using the mappedBy attribute of the @ManyToMany annotation. This establishes the inverse side of the relationship. The database model for these entities can be visualized as follows: +--------------+ +-------------------+ +------------------+ | Student | | Course | | course_signed | +--------------+ +-------------------+ +------------------+ | id (PK) |<--->| id (PK) | | student_id (FK) | | name | | name | | course_id (FK) | +--------------+ +-------------------+ +------------------+ The Student and Course tables represent the entities, and the course_signed table serves as the join table for the many-to-many relationship between them. The student_id and course_id columns in the course_signed table establish the associations between students and courses. This database model allows for a many-to-many relationship between students and courses, enabling students to be enrolled in multiple courses, and courses to have multiple students. In order to manipulate the database entities two different repository classes were provided. The Spring Data JPA interfaces JpaRepository and JpaRepository are part of the Spring Data JPA framework, which provides a set of abstractions and utilities for working with relational databases using the Java Persistence API (JPA). These interfaces provide a high-level abstraction for performing common database operations on the Course and Student entities without the need for writing boilerplate code. Some benefits of using Spring Data JPA interfaces include: Query Methods: Spring Data JPA interfaces allow you to define custom query methods by simply declaring method signatures. The framework automatically generates the necessary SQL queries based on the method names, reducing the need for manual query creation. CRUD Operations: The interfaces provide methods for performing CRUD (Create, Read, Update, Delete) operations on the entities. This eliminates the need to manually write SQL statements or implement these operations yourself. Transaction Management: The interfaces integrate with Spring's transaction management capabilities, ensuring that database operations are performed within a transactional context. Especially the third feature is important from the perspective of this work, as it allows us to test how multiple transactional operations will work on the system. Both repositories are used in a SchoolService class. The SchoolService class is a Spring service component that provides business logic and acts as an intermediary between the controller layer and the data access layer (repositories) in the application. It exposes methods for creating students and courses ( createStudent and createCourse ). These methods utilize the respective repository's save method to persist the entities in the database. The getStudents, getCourses, and getStudentsForCourse methods utilize the repository's query methods to retrieve entities from the database. The findAll method fetches all students or courses, while findById retrieves a specific student or course based on the provided ID. Spring manages transactions implicitly for the service methods. Each method operates within a transactional context, ensuring data consistency and integrity. @RequiredArgsConstructor @Service public class SchoolService { private final CourseRepository courseRepository; private final StudentRepository studentRepository; public Student createStudent(Student student) { return studentRepository.save(student); } public Course createCourse(Course course) { return courseRepository.save(course); } public Student addCourseForStudent(Long studentId, Long courseId) { Course courseFound = courseRepository.findById(courseId).orElseThrow(); Student studentFound = studentRepository.findById(studentId).orElseThrow(); studentFound.courses.add(courseFound); return studentRepository.save(studentFound); } public List<Student> getStudents() { return studentRepository.findAll(); } public List<Course> getCourses() { return courseRepository.findAll(); } public List<Student> getStudentsForCourse(Long courseId) { Course course = courseRepository.findById(courseId).orElseThrow(); return course.students; } public void clearAll() { studentRepository.deleteAll(); } } The final implementation layer for the transactional db testing feature is a SchoolController class. The SchoolController class is a Spring REST controller responsible for handling HTTP requests related to the school entities. It receives requests from clients, invokes the appropriate methods in the SchoolService, and returns the response. Each method in the controller is annotated with an HTTP method mapping annotation (@PostMapping, @GetMapping). These annotations specify the URL path for the corresponding request and the HTTP method to be used. Methods like addStudent and addCourse use the @RequestBody annotation to deserialize the request body JSON into Java objects (Student and Course, respectively). Methods like addCourseForStudent and getAllStudentsForCourse utilize path variables (@PathVariable) to extract dynamic values from the URL path. The controller uses a logger (slf4j) to log important information related to the incoming requests and performed operations. The @Transactional annotation is used on the getAllStudentsForCourse method to indicate that the method operates within a transactional context. This ensures data consistency and integrity when retrieving students for a specific course. The controller helps in separating the concerns between handling the web layer and implementing the business logic in the service layer. By using Spring annotations, the SchoolController simplifies the development of RESTful APIs and promotes the adoption of best practices in building web services. @RestController @RequiredArgsConstructor @Slf4j public class SchoolController { private final SchoolService schoolService; @PostMapping(\"/create/student\") public Student addStudent(@RequestBody Student student) { log.info(\"Adding a student : \" + student); return schoolService.createStudent(student); } @PostMapping(\"/create/course\") public Course addCourse(@RequestBody Course course) { log.info(\"Adding a course : \" + course); return schoolService.createCourse(course); } @PostMapping(\"/add/{studentId}/{courseId}\") public Student addCourseForStudent(@PathVariable Long studentId, @PathVariable Long courseId) { log.info(\"Adding a course : \" + courseId + \" for student \" + studentId); return schoolService.addCourseForStudent(studentId, courseId); } @GetMapping(\"/students/all\") public List<Student> getAllStudents() { log.info(\"Retrieving students ...\"); return schoolService.getStudents(); } @GetMapping(\"/courses/all\") public List<Course> getAllCourses() { log.info(\"Retrieving courses ...\"); return schoolService.getCourses(); } @GetMapping(\"/courses/{courseId}/students\") @Transactional public List<Student> getAllStudentsForCourse(@PathVariable Long courseId) { log.info(\"Students for course : \" + courseId); return schoolService.getStudentsForCourse(courseId); } @PostMapping(\"/clear\") public String clearAll() { schoolService.clearAll(); return \"DB cleared\"; } }","title":"Implementation"},{"location":"cluster-probe/implementation/#implementation","text":"","title":"Implementation"},{"location":"cluster-probe/implementation/#introduction","text":"Cluster Probe is written in Java 17 , and it uses the Spring Boot Framework . Each feature's code is stored in its own java package. All packages expose a service class that can be injected as a dependency to the Controller, the REST api web layer class. Any error that has occurred during the test is handled by the ErrorHandler class that specifies an appropriate message and returns the error response entity.","title":"Introduction"},{"location":"cluster-probe/implementation/#file-operations","text":"FileOperations service is using java standard development kit libraries to implement the creation, update and the deletion for files. Those operations are performed based on the data specified in FileSystemSpecification class object. there is only one public method implemented for the FileOperationsService all the files created will be stored under the projects directory in the 'test' folder, then the number of files will be created based on parameters specified in the specification object. Specification class If the fileContent parameter will be null or empty no write operations will be performed. After all the operations are performed files and the 'test' directory are deleted: The FileService can be invoked from the Controller class by calling an HTTP POST method. @PostMapping(\"/api/file-operations\") public String createFiles(@RequestBody FileSystemSpecification spec) { fileOperationsService.testFileSystem(spec); return String.format(\"File operations performed based on spec %s\", spec); } Here, createFiles method handles the POST request for the file-operations endpoint, accepts a FileSystemSpecification object in the request body, and delegates the file operations logic to the fileOperationsService . It responds with a message confirming the successful execution of the file operations based on the provided specification.","title":"File operations"},{"location":"cluster-probe/implementation/#stress-ng","text":"In order to implement the stress ng jobs invocation, TestService creates an appropriate stress ng job based on the TestSpecification parameters, not all parameters are required so mostly TestService class implements the logic for creating the job accurately. Specification class for each parameter in the specification there is an according mapping method in the service class that based on whether the parameter is correct or not it creates an appropriate part of the command. There is also an option for receiving a plain command as a string, if this one is available and the isCommand flag is set to true the rest of the parameters will be ignored. Whenever the durationInSeconds parameter will not be available there is a default 24 hours value set in the mapTestTimeCommand : The TestService can be invoked from the Controller class by calling an HTTP POST method. @PostMapping(\"/api/job\") public String startJob(@RequestBody TestSpecification specification) { log.info(\"Received job specification: \" + specification); boolean jobStarted = testService.startJobSpecification(specification); if (jobStarted) { log.info(\"Job started for specification: \" + specification); } else { log.error(\"Job failed to start for specification: \" + specification); } return \"Job started\"; } The method delegates the processing of the request to the testService by calling the startJobSpecification method and passing the specification object. Depending on the result returned by the service layer, the method logs appropriate messages. If the job is successfully started, it logs that the job has started. If the job fails to start, it logs an error message. The method returns a simple string message indicating that the job has started.","title":"Stress ng"},{"location":"cluster-probe/implementation/#transactional","text":"In order to give the users an availability to test multiple database connections with transactional operations, a small example for the school domain have been created. It represents a classical model of a student - course many to many relationship. The Student class represents a student in the educational domain. It is annotated with @Entity to indicate that it is a JPA entity mapped to a database table. The class has the following attributes: id : A unique identifier for the student, annotated with @Id and @GeneratedValue to indicate that it is the primary key and its value is automatically generated. name : The name of the student, annotated with @Column to specify the mapping to the corresponding column in the database table. courses : A many-to-many relationship with the Course class, representing the courses that the student is enrolled in. The relationship is defined using the @ManyToMany annotation, and the mapping is managed through a join table named \"course_signed\". The Course class represents a course offered in the educational domain. It has similar annotations and attributes as the Student class: id : The unique identifier for the course. name : The name of the course. students : A bidirectional many-to-many relationship with the Student class. It is mapped by the courses attribute in the Student class using the mappedBy attribute of the @ManyToMany annotation. This establishes the inverse side of the relationship. The database model for these entities can be visualized as follows: +--------------+ +-------------------+ +------------------+ | Student | | Course | | course_signed | +--------------+ +-------------------+ +------------------+ | id (PK) |<--->| id (PK) | | student_id (FK) | | name | | name | | course_id (FK) | +--------------+ +-------------------+ +------------------+ The Student and Course tables represent the entities, and the course_signed table serves as the join table for the many-to-many relationship between them. The student_id and course_id columns in the course_signed table establish the associations between students and courses. This database model allows for a many-to-many relationship between students and courses, enabling students to be enrolled in multiple courses, and courses to have multiple students. In order to manipulate the database entities two different repository classes were provided. The Spring Data JPA interfaces JpaRepository and JpaRepository are part of the Spring Data JPA framework, which provides a set of abstractions and utilities for working with relational databases using the Java Persistence API (JPA). These interfaces provide a high-level abstraction for performing common database operations on the Course and Student entities without the need for writing boilerplate code. Some benefits of using Spring Data JPA interfaces include: Query Methods: Spring Data JPA interfaces allow you to define custom query methods by simply declaring method signatures. The framework automatically generates the necessary SQL queries based on the method names, reducing the need for manual query creation. CRUD Operations: The interfaces provide methods for performing CRUD (Create, Read, Update, Delete) operations on the entities. This eliminates the need to manually write SQL statements or implement these operations yourself. Transaction Management: The interfaces integrate with Spring's transaction management capabilities, ensuring that database operations are performed within a transactional context. Especially the third feature is important from the perspective of this work, as it allows us to test how multiple transactional operations will work on the system. Both repositories are used in a SchoolService class. The SchoolService class is a Spring service component that provides business logic and acts as an intermediary between the controller layer and the data access layer (repositories) in the application. It exposes methods for creating students and courses ( createStudent and createCourse ). These methods utilize the respective repository's save method to persist the entities in the database. The getStudents, getCourses, and getStudentsForCourse methods utilize the repository's query methods to retrieve entities from the database. The findAll method fetches all students or courses, while findById retrieves a specific student or course based on the provided ID. Spring manages transactions implicitly for the service methods. Each method operates within a transactional context, ensuring data consistency and integrity. @RequiredArgsConstructor @Service public class SchoolService { private final CourseRepository courseRepository; private final StudentRepository studentRepository; public Student createStudent(Student student) { return studentRepository.save(student); } public Course createCourse(Course course) { return courseRepository.save(course); } public Student addCourseForStudent(Long studentId, Long courseId) { Course courseFound = courseRepository.findById(courseId).orElseThrow(); Student studentFound = studentRepository.findById(studentId).orElseThrow(); studentFound.courses.add(courseFound); return studentRepository.save(studentFound); } public List<Student> getStudents() { return studentRepository.findAll(); } public List<Course> getCourses() { return courseRepository.findAll(); } public List<Student> getStudentsForCourse(Long courseId) { Course course = courseRepository.findById(courseId).orElseThrow(); return course.students; } public void clearAll() { studentRepository.deleteAll(); } } The final implementation layer for the transactional db testing feature is a SchoolController class. The SchoolController class is a Spring REST controller responsible for handling HTTP requests related to the school entities. It receives requests from clients, invokes the appropriate methods in the SchoolService, and returns the response. Each method in the controller is annotated with an HTTP method mapping annotation (@PostMapping, @GetMapping). These annotations specify the URL path for the corresponding request and the HTTP method to be used. Methods like addStudent and addCourse use the @RequestBody annotation to deserialize the request body JSON into Java objects (Student and Course, respectively). Methods like addCourseForStudent and getAllStudentsForCourse utilize path variables (@PathVariable) to extract dynamic values from the URL path. The controller uses a logger (slf4j) to log important information related to the incoming requests and performed operations. The @Transactional annotation is used on the getAllStudentsForCourse method to indicate that the method operates within a transactional context. This ensures data consistency and integrity when retrieving students for a specific course. The controller helps in separating the concerns between handling the web layer and implementing the business logic in the service layer. By using Spring annotations, the SchoolController simplifies the development of RESTful APIs and promotes the adoption of best practices in building web services. @RestController @RequiredArgsConstructor @Slf4j public class SchoolController { private final SchoolService schoolService; @PostMapping(\"/create/student\") public Student addStudent(@RequestBody Student student) { log.info(\"Adding a student : \" + student); return schoolService.createStudent(student); } @PostMapping(\"/create/course\") public Course addCourse(@RequestBody Course course) { log.info(\"Adding a course : \" + course); return schoolService.createCourse(course); } @PostMapping(\"/add/{studentId}/{courseId}\") public Student addCourseForStudent(@PathVariable Long studentId, @PathVariable Long courseId) { log.info(\"Adding a course : \" + courseId + \" for student \" + studentId); return schoolService.addCourseForStudent(studentId, courseId); } @GetMapping(\"/students/all\") public List<Student> getAllStudents() { log.info(\"Retrieving students ...\"); return schoolService.getStudents(); } @GetMapping(\"/courses/all\") public List<Course> getAllCourses() { log.info(\"Retrieving courses ...\"); return schoolService.getCourses(); } @GetMapping(\"/courses/{courseId}/students\") @Transactional public List<Student> getAllStudentsForCourse(@PathVariable Long courseId) { log.info(\"Students for course : \" + courseId); return schoolService.getStudentsForCourse(courseId); } @PostMapping(\"/clear\") public String clearAll() { schoolService.clearAll(); return \"DB cleared\"; } }","title":"Transactional"},{"location":"cluster-probe/requirements/","text":"Introduction As it was mentioned before Cluster Probe is meant to run on any container, however its main goal is to be a load receiver for Kubernetes cluster especially for the reCluster architecture. In order to make the testing process most effective there are several functional and non-functional requirements that were identified, they also constitute the most important features of the Cluster Probe. Functional requirements The system shall produce workloads based on a defined and measurable load specifications The system shall be able to produce intense I/O operations. The system shall be able to produce long-lasting or time specified intensive cpu tasks. The system shall be able to produce multiple asynchronous computing tasks. The system shall be able to receive data through the HTTP protocol. Non-functional requirements The system shall be capable of handling a high volume of concurrent requests for each endpoint efficiently. The system shall handle errors gracefully and provide appropriate error messages to clients. The system API shall be well-documented and contain an intuitive interface, allowing users to understand and interact with the endpoints easily.","title":"Requirements"},{"location":"cluster-probe/requirements/#introduction","text":"As it was mentioned before Cluster Probe is meant to run on any container, however its main goal is to be a load receiver for Kubernetes cluster especially for the reCluster architecture. In order to make the testing process most effective there are several functional and non-functional requirements that were identified, they also constitute the most important features of the Cluster Probe.","title":"Introduction"},{"location":"cluster-probe/requirements/#functional-requirements","text":"The system shall produce workloads based on a defined and measurable load specifications The system shall be able to produce intense I/O operations. The system shall be able to produce long-lasting or time specified intensive cpu tasks. The system shall be able to produce multiple asynchronous computing tasks. The system shall be able to receive data through the HTTP protocol.","title":"Functional requirements"},{"location":"cluster-probe/requirements/#non-functional-requirements","text":"The system shall be capable of handling a high volume of concurrent requests for each endpoint efficiently. The system shall handle errors gracefully and provide appropriate error messages to clients. The system API shall be well-documented and contain an intuitive interface, allowing users to understand and interact with the endpoints easily.","title":"Non-functional requirements"},{"location":"cluster-probe/user-guide/","text":"To run a local Kubernetes (k8s) cluster with Cluster Probe, follow these steps: Install Minikube: Before deploying the cluster, make sure you have Minikube installed. Minikube allows you to run a local Kubernetes cluster on your machine. Start Minikube: Launch the Minikube cluster by running minikube start in your terminal. This will create a Kubernetes instance on your local machine. Enable Metrics Server: Metrics Server is required for autoscaling. Enable it by executing minikube addons enable metrics-server . Apply Kubernetes Configuration Files: In the Cluster Probe repository, you will find two Kubernetes files, deployment.yaml and service.yaml . Apply these files to deploy the Cluster Probe image in your local Kubernetes cluster. The Docker image used in the deployment is defined in the root directory's Dockerfile . ```dockerfile # Dockerfile FROM bellsoft/liberica-openjdk-alpine-musl:17 VOLUME /tmp RUN apk update && apk add stress-ng COPY target/*.jar app.jar ENTRYPOINT [\"java\",\"-jar\",\"/app.jar\"] ``` This docker images is available publicly at the docker hub registry To deploy the application, run the following commands: kubectl apply -f deployment.yaml kubectl apply -f service.yaml Expose the Service: To test the service on your local machine, you need to expose it. Run minikube service cluster-probe-service to expose the Cluster Probe service. Monitor the Cluster: In another terminal, start the Minikube dashboard using minikube dashboard . The dashboard provides a graphical interface to monitor the cluster's resources. Configure Autoscaling: Optionally, you can set up autoscaling for the Cluster Probe deployment. For example, to configure autoscaling based on CPU usage, use the following command: kubectl autoscale deployment cluster-probe --cpu-percent=30 --min=1 --max=10 With these steps, you have set up a local Kubernetes cluster with Cluster Probe deployed. You can now test and monitor the application using the Minikube dashboard and verify the autoscaling behavior if configured.","title":"User guide"},{"location":"overview/introduction/","text":"Introduction Welcome to the documentation of Cluster Probe and Probe Builder. This documentation provides comprehensive information and guidance on the Cluster Probe and Probe Builder projects, which are software packages designed for testing scalability and performance in Kubernetes clusters. The main objective of these projects is to create a software-based solution for testing typical cloud service use cases within the context of the reCluster project, a self-aware cloud orchestration system. The reCluster architecture presents a sustainable approach to data centre operations, leveraging upcycled hardware and prioritizing resource reduction. It seeks to minimize the environmental impact associated with computing while still offering viable computing capabilities. The main goal of the both Cluster Probe and Probe Builder is to be deployed on the reCluster and truly verify the computational capabilities of the system. However, Cluster Probe and Probe Builder offer a robust and user-friendly solution for testing the capabilities and limits of all the Kubernetes clusters. They enable application developers, system administrators, and testers to assess the performance, resilience, and efficiency of their cluster deployments. Cluster Probe allows users to conduct thorough performance tests by simulating real-world scenarios and evaluating the cluster's response under various workloads. This tool provides valuable insights into the system's capacity, identifies bottlenecks, and assesses scalability potential. On the other hand, Probe Builder , complements Cluster Probe by providing a convenient way to create custom test cases and scenarios. It utilizes JMeter's Java API to generate and control the load based on the specified input parameters. This allows users to define specific test criteria and performance goals, enabling fine-grained control over the testing process. The documentation is structured to cover the requirements, architecture, implementation, testing methodologies, API specifications, and user guidance for both Cluster Probe and Probe Builder. Whether you are new to these projects or seeking advanced configuration options, this documentation serves as a comprehensive resource for effectively utilizing these testing tools. Requirements and Dependencies Both projects, Cluster Probe and Probe Builder, are developed using the Java programming language. Familiarity with containerization concepts and Docker is required. Additionally, code snippets and examples will incorporate Kubernetes concepts. To run the local examples, ensure you have the following dependencies installed: JDK 17 : Java Development Kit version 17. Docker : Containerization platform for building and managing containers. Minikube : Lightweight Kubernetes implementation for local testing and development. Git : Version control system for code repositories. Postman : API development and testing tool (recommended for testing the API endpoints). Please ensure that you have the appropriate versions of these dependencies installed and configured correctly before proceeding with the projects. Note: Although Probe Builder is based on JMeter, it is not necessary to have in-depth knowledge of JMeter to utilize its basic functionalities. However, it is required to have JMeter installed. You can download JMeter from the official JMeter download page . Additionally, a GitHub account is recommended to access the code repositories and version control features. By having the above dependencies set up properly, you will be ready to explore and utilize the features and functionalities of Cluster Probe and Probe Builder effectively. Key features Cluster Probe and Probe Builder are designed to provide a system for defining and executing test loads based on a set of common and identified use cases. These use cases encompass various scenarios such as long-lasting intense CPU operations, file system operations, JSON data payloads, and file uploads. Probe Builder offers a user-friendly interface that enables users to specify these use cases and generate JMeter test plans. These test plans are executed and loaded onto a designated Kubernetes (k8s) cluster. After the tests are performed, Probe Builder collects the data and generates reports in the form of plots and CSV files. These reports provide valuable insights into the performance of the tested cluster. Cluster Probe, on the other hand, functions as a container running within k8s pods. It implements a REST API service capable of receiving different use cases, as mentioned earlier. For instance, it can handle CPU load scenarios using stress-ng or perform file operations. The diagram below illustrates the overall architecture and interaction between the components. You can find more detailed information about both of the projects in the following sections probe-builder , cluster-probe","title":"Introduction"},{"location":"overview/introduction/#introduction","text":"Welcome to the documentation of Cluster Probe and Probe Builder. This documentation provides comprehensive information and guidance on the Cluster Probe and Probe Builder projects, which are software packages designed for testing scalability and performance in Kubernetes clusters. The main objective of these projects is to create a software-based solution for testing typical cloud service use cases within the context of the reCluster project, a self-aware cloud orchestration system. The reCluster architecture presents a sustainable approach to data centre operations, leveraging upcycled hardware and prioritizing resource reduction. It seeks to minimize the environmental impact associated with computing while still offering viable computing capabilities. The main goal of the both Cluster Probe and Probe Builder is to be deployed on the reCluster and truly verify the computational capabilities of the system. However, Cluster Probe and Probe Builder offer a robust and user-friendly solution for testing the capabilities and limits of all the Kubernetes clusters. They enable application developers, system administrators, and testers to assess the performance, resilience, and efficiency of their cluster deployments. Cluster Probe allows users to conduct thorough performance tests by simulating real-world scenarios and evaluating the cluster's response under various workloads. This tool provides valuable insights into the system's capacity, identifies bottlenecks, and assesses scalability potential. On the other hand, Probe Builder , complements Cluster Probe by providing a convenient way to create custom test cases and scenarios. It utilizes JMeter's Java API to generate and control the load based on the specified input parameters. This allows users to define specific test criteria and performance goals, enabling fine-grained control over the testing process. The documentation is structured to cover the requirements, architecture, implementation, testing methodologies, API specifications, and user guidance for both Cluster Probe and Probe Builder. Whether you are new to these projects or seeking advanced configuration options, this documentation serves as a comprehensive resource for effectively utilizing these testing tools.","title":"Introduction"},{"location":"overview/introduction/#requirements-and-dependencies","text":"Both projects, Cluster Probe and Probe Builder, are developed using the Java programming language. Familiarity with containerization concepts and Docker is required. Additionally, code snippets and examples will incorporate Kubernetes concepts. To run the local examples, ensure you have the following dependencies installed: JDK 17 : Java Development Kit version 17. Docker : Containerization platform for building and managing containers. Minikube : Lightweight Kubernetes implementation for local testing and development. Git : Version control system for code repositories. Postman : API development and testing tool (recommended for testing the API endpoints). Please ensure that you have the appropriate versions of these dependencies installed and configured correctly before proceeding with the projects. Note: Although Probe Builder is based on JMeter, it is not necessary to have in-depth knowledge of JMeter to utilize its basic functionalities. However, it is required to have JMeter installed. You can download JMeter from the official JMeter download page . Additionally, a GitHub account is recommended to access the code repositories and version control features. By having the above dependencies set up properly, you will be ready to explore and utilize the features and functionalities of Cluster Probe and Probe Builder effectively.","title":"Requirements and Dependencies"},{"location":"overview/introduction/#key-features","text":"Cluster Probe and Probe Builder are designed to provide a system for defining and executing test loads based on a set of common and identified use cases. These use cases encompass various scenarios such as long-lasting intense CPU operations, file system operations, JSON data payloads, and file uploads. Probe Builder offers a user-friendly interface that enables users to specify these use cases and generate JMeter test plans. These test plans are executed and loaded onto a designated Kubernetes (k8s) cluster. After the tests are performed, Probe Builder collects the data and generates reports in the form of plots and CSV files. These reports provide valuable insights into the performance of the tested cluster. Cluster Probe, on the other hand, functions as a container running within k8s pods. It implements a REST API service capable of receiving different use cases, as mentioned earlier. For instance, it can handle CPU load scenarios using stress-ng or perform file operations. The diagram below illustrates the overall architecture and interaction between the components. You can find more detailed information about both of the projects in the following sections probe-builder , cluster-probe","title":"Key features"},{"location":"probe-builder/design/","text":"Design The design of the Probe Builder is geared toward providing a user-friendly tool designed to streamline the creation and reporting of JMeter service tests, specifically tailored for the integration with the Cluster Probe software running in a Kubernetes cluster. The UI allows users to effortlessly configure JMeter tests by communicating with the Spring Boot REST API Controller, which then delegates tasks to the JMeter Service. The JMeter Service interacts with JMeter using the JMeter Java API, executing the specified tests and generating CSV files containing test data. After test completion, the JMeter Service interacts with the JMeter Repository and the Report Creator. The JMeter Repository stores test-related information, while the Report Creator generates informative plots for easy test result analysis. Data is passed by through different components with the JmeterResultData and JMeterSpecification classes, thanks to that data exchange between different layers is well organized and cohesive. In designing Probe Builder, the focus was put on modularity and simplicity by ensuring clear separation of concerns and well-defined architectural boundaries. This approach promotes code usability and fosters a more organized workflow. By leveraging JMeter's Java API, Probe Builder integrates with JMeter, enabling the tool to leverage JMeter's testing capabilities for flexible and scalable testing scenarios.","title":"Design"},{"location":"probe-builder/design/#design","text":"The design of the Probe Builder is geared toward providing a user-friendly tool designed to streamline the creation and reporting of JMeter service tests, specifically tailored for the integration with the Cluster Probe software running in a Kubernetes cluster. The UI allows users to effortlessly configure JMeter tests by communicating with the Spring Boot REST API Controller, which then delegates tasks to the JMeter Service. The JMeter Service interacts with JMeter using the JMeter Java API, executing the specified tests and generating CSV files containing test data. After test completion, the JMeter Service interacts with the JMeter Repository and the Report Creator. The JMeter Repository stores test-related information, while the Report Creator generates informative plots for easy test result analysis. Data is passed by through different components with the JmeterResultData and JMeterSpecification classes, thanks to that data exchange between different layers is well organized and cohesive. In designing Probe Builder, the focus was put on modularity and simplicity by ensuring clear separation of concerns and well-defined architectural boundaries. This approach promotes code usability and fosters a more organized workflow. By leveraging JMeter's Java API, Probe Builder integrates with JMeter, enabling the tool to leverage JMeter's testing capabilities for flexible and scalable testing scenarios.","title":"Design"},{"location":"probe-builder/implementation/","text":"Implementation Introduction ProbeBuilder simillary as ClusterProbe is written in Java 17 , and it also uses the Spring Boot Framework . More than that however there are few dependencies that are worth mentioning regarding the implementation part of the probe builder: JMeter Java API: Used for creating and executing load tests within the application. Tablesaw: Employed for data manipulation and analysis in a tabular format. Maven: The build tool utilized for managing dependencies and project build processes. Vanilla JavaScript and HTML: Used to build a simple yet effective frontend for the application. The main server side code is implemented in the jmeter package that stores all the important classes connected with the jmeter integration as well as reporting and test results data management. JmeterService Most of the methods in this class are focused around the Jmeter test preparation and execution, there are several methods that use directly the Jmeter Java API like: @NotNull private static TestPlan buildTestPlan() { TestPlan testPlan = new TestPlan(\"Create JMeter Script From Java Code\"); testPlan.setProperty(TestElement.TEST_CLASS, TestPlan.class.getName()); testPlan.setProperty(TestElement.GUI_CLASS, TestPlanGui.class.getName()); testPlan.setUserDefinedVariables((Arguments) new ArgumentsPanel().createTestElement()); return testPlan; } or @NotNull private static ThreadGroup buildThreadGroup(JmeterSpecification spec, LoopController loopController) { ThreadGroup threadGroup = new ThreadGroup(); threadGroup.setName(\"Thread Group\"); threadGroup.setNumThreads(spec.getNumberOfThreads()); threadGroup.setRampUp(spec.getRampUpPeriod()); threadGroup.setSamplerController(loopController); threadGroup.setProperty(TestElement.TEST_CLASS, ThreadGroup.class.getName()); threadGroup.setProperty(TestElement.GUI_CLASS, ThreadGroupGui.class.getName()); return threadGroup; } where you can see that JmeterSpecification parameters are passed to some fields of the threadGroup configuration. All the jmeter builder methods are used in the main public jmeterStart method. // First HTTP Sampler HTTPSamplerProxy sampler = buildSampler(spec); // Loop Controller LoopController loopController = buildLoopController(); // Thread Group ThreadGroup threadGroup = buildThreadGroup(spec, loopController); // Test Plan TestPlan testPlan = buildTestPlan(); after all the tests elements are set up, there is a thread prepared and executed with the testPlan constructed: Thread thread = new Thread(() -> { String date = LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")); repo.save(new JmeterResultData(date, fileName + \".jtl\", fileName + \".jmx\", String.valueOf(spec.getNumberOfThreads()), String.valueOf(spec.getRampUpPeriod()), spec.getPath(), false )); jmeter.run(); ReportCreator.create(fileName+\".csv\"); repo.save(new JmeterResultData(date, fileName + \".csv\", fileName + \".jmx\", String.valueOf(spec.getNumberOfThreads()), String.valueOf(spec.getRampUpPeriod()), spec.getPath(), true )); }); JmeterResultData for a test is saved just before the test invocation as well as just after it. There is also a report created with a static ReportCreator.create() method call. Report Creator ReportCreator class takes advantage of the tablesaw library for plotting. The main method for this class is a create method. This method is a static utility method that takes a file path as input and generates a report based on the data present in the file. The method reads data from a CSV file specified by the file parameter and loads it into a Tablesaw Table object. Later on latency and success categories are calculated. Then the method creates plots: \"Errors and Success\" plot: A horizontal bar plot showing the count of successful and failed operations. \"Latency over time\" plot: A time series plot showing how the latency (response time) changes over time (represented by the \"count\" column). At the end the method generates an HTML report file containing the two plots using the ReportData class. The report file is named based on the input CSV file but with a \".html\" extension. public static ReportData create(String file) { try { Table table = Table.read().file(new File(file)); table.addColumns(DoubleColumn.create(\"count\", IntStream.range(1, table.rowCount() + 1).toArray())); DoubleColumn latencySeconds = table.intColumn(\"Latency\").divide(1000).setName(\"Latency seconds\"); table.addColumns(latencySeconds); CategoricalColumn<?> success = table.categoricalColumn(\"success\"); Table successCategory = success.countByCategory(); Figure errrorsAndSuccessPlot = HorizontalBarPlot.create(\"Errors and success\", successCategory, \"Category\", \"Count\"); Figure latencyOverTime = TimeSeriesPlot.create(\"Latency over time\", table, \"count\", \"Latency seconds\"); return new ReportData( createHTMLFile(\"report\" + file.replaceAll(\".csv\", \".html\"), Arrays.asList(latencyOverTime, errrorsAndSuccessPlot)), String.valueOf(getErrorCount(successCategory)), getLatencyMean(table) ); } catch (IOException e) { throw new IllegalStateException(\"Failed to create a report for \" + file); } } Controller Similar as for to the cluster-probe, probe-builder exposes its Rest API with a Spring Boot RestController class. There are two main methods for this Controller : jmeterRun - This method is the handler for the \"/run\" POST endpoint. It takes a JmeterSpecification object as input, which is deserialized from the request body using @RequestBody. If the spec is not provided (null), it creates a default JmeterSpecification object and starts the JMeter test with the default values. Otherwise, it starts the JMeter test with the specified JmeterSpecification. results - This method is the handler for the \"/results\" GET endpoint. It returns a Map of JmeterResultData, which contains the results of JMeter tests performed by the jmeterService. @RestController @RequiredArgsConstructor public class Controller { private final JmeterService jmeterService; @PostMapping(\"/run\") @CrossOrigin public void jmeterRun(@RequestBody JmeterSpecification spec) throws IOException { if (spec == null) { jmeterService.jmeterStart(new JmeterSpecification(100, 20, \"localhost\", 8080, \"/api/load\", \"POST\", \"{\\\"key1\\\":\\\"value1\\\"}\")); } else { jmeterService.jmeterStart(spec); } } @GetMapping(\"/results\") @CrossOrigin public Map<String, JmeterResultData> results() { return jmeterService.getAllResults(); } }","title":"Implementation"},{"location":"probe-builder/implementation/#implementation","text":"","title":"Implementation"},{"location":"probe-builder/implementation/#introduction","text":"ProbeBuilder simillary as ClusterProbe is written in Java 17 , and it also uses the Spring Boot Framework . More than that however there are few dependencies that are worth mentioning regarding the implementation part of the probe builder: JMeter Java API: Used for creating and executing load tests within the application. Tablesaw: Employed for data manipulation and analysis in a tabular format. Maven: The build tool utilized for managing dependencies and project build processes. Vanilla JavaScript and HTML: Used to build a simple yet effective frontend for the application. The main server side code is implemented in the jmeter package that stores all the important classes connected with the jmeter integration as well as reporting and test results data management.","title":"Introduction"},{"location":"probe-builder/implementation/#jmeterservice","text":"Most of the methods in this class are focused around the Jmeter test preparation and execution, there are several methods that use directly the Jmeter Java API like: @NotNull private static TestPlan buildTestPlan() { TestPlan testPlan = new TestPlan(\"Create JMeter Script From Java Code\"); testPlan.setProperty(TestElement.TEST_CLASS, TestPlan.class.getName()); testPlan.setProperty(TestElement.GUI_CLASS, TestPlanGui.class.getName()); testPlan.setUserDefinedVariables((Arguments) new ArgumentsPanel().createTestElement()); return testPlan; } or @NotNull private static ThreadGroup buildThreadGroup(JmeterSpecification spec, LoopController loopController) { ThreadGroup threadGroup = new ThreadGroup(); threadGroup.setName(\"Thread Group\"); threadGroup.setNumThreads(spec.getNumberOfThreads()); threadGroup.setRampUp(spec.getRampUpPeriod()); threadGroup.setSamplerController(loopController); threadGroup.setProperty(TestElement.TEST_CLASS, ThreadGroup.class.getName()); threadGroup.setProperty(TestElement.GUI_CLASS, ThreadGroupGui.class.getName()); return threadGroup; } where you can see that JmeterSpecification parameters are passed to some fields of the threadGroup configuration. All the jmeter builder methods are used in the main public jmeterStart method. // First HTTP Sampler HTTPSamplerProxy sampler = buildSampler(spec); // Loop Controller LoopController loopController = buildLoopController(); // Thread Group ThreadGroup threadGroup = buildThreadGroup(spec, loopController); // Test Plan TestPlan testPlan = buildTestPlan(); after all the tests elements are set up, there is a thread prepared and executed with the testPlan constructed: Thread thread = new Thread(() -> { String date = LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")); repo.save(new JmeterResultData(date, fileName + \".jtl\", fileName + \".jmx\", String.valueOf(spec.getNumberOfThreads()), String.valueOf(spec.getRampUpPeriod()), spec.getPath(), false )); jmeter.run(); ReportCreator.create(fileName+\".csv\"); repo.save(new JmeterResultData(date, fileName + \".csv\", fileName + \".jmx\", String.valueOf(spec.getNumberOfThreads()), String.valueOf(spec.getRampUpPeriod()), spec.getPath(), true )); }); JmeterResultData for a test is saved just before the test invocation as well as just after it. There is also a report created with a static ReportCreator.create() method call.","title":"JmeterService"},{"location":"probe-builder/implementation/#report-creator","text":"ReportCreator class takes advantage of the tablesaw library for plotting. The main method for this class is a create method. This method is a static utility method that takes a file path as input and generates a report based on the data present in the file. The method reads data from a CSV file specified by the file parameter and loads it into a Tablesaw Table object. Later on latency and success categories are calculated. Then the method creates plots: \"Errors and Success\" plot: A horizontal bar plot showing the count of successful and failed operations. \"Latency over time\" plot: A time series plot showing how the latency (response time) changes over time (represented by the \"count\" column). At the end the method generates an HTML report file containing the two plots using the ReportData class. The report file is named based on the input CSV file but with a \".html\" extension. public static ReportData create(String file) { try { Table table = Table.read().file(new File(file)); table.addColumns(DoubleColumn.create(\"count\", IntStream.range(1, table.rowCount() + 1).toArray())); DoubleColumn latencySeconds = table.intColumn(\"Latency\").divide(1000).setName(\"Latency seconds\"); table.addColumns(latencySeconds); CategoricalColumn<?> success = table.categoricalColumn(\"success\"); Table successCategory = success.countByCategory(); Figure errrorsAndSuccessPlot = HorizontalBarPlot.create(\"Errors and success\", successCategory, \"Category\", \"Count\"); Figure latencyOverTime = TimeSeriesPlot.create(\"Latency over time\", table, \"count\", \"Latency seconds\"); return new ReportData( createHTMLFile(\"report\" + file.replaceAll(\".csv\", \".html\"), Arrays.asList(latencyOverTime, errrorsAndSuccessPlot)), String.valueOf(getErrorCount(successCategory)), getLatencyMean(table) ); } catch (IOException e) { throw new IllegalStateException(\"Failed to create a report for \" + file); } }","title":"Report Creator"},{"location":"probe-builder/implementation/#controller","text":"Similar as for to the cluster-probe, probe-builder exposes its Rest API with a Spring Boot RestController class. There are two main methods for this Controller : jmeterRun - This method is the handler for the \"/run\" POST endpoint. It takes a JmeterSpecification object as input, which is deserialized from the request body using @RequestBody. If the spec is not provided (null), it creates a default JmeterSpecification object and starts the JMeter test with the default values. Otherwise, it starts the JMeter test with the specified JmeterSpecification. results - This method is the handler for the \"/results\" GET endpoint. It returns a Map of JmeterResultData, which contains the results of JMeter tests performed by the jmeterService. @RestController @RequiredArgsConstructor public class Controller { private final JmeterService jmeterService; @PostMapping(\"/run\") @CrossOrigin public void jmeterRun(@RequestBody JmeterSpecification spec) throws IOException { if (spec == null) { jmeterService.jmeterStart(new JmeterSpecification(100, 20, \"localhost\", 8080, \"/api/load\", \"POST\", \"{\\\"key1\\\":\\\"value1\\\"}\")); } else { jmeterService.jmeterStart(spec); } } @GetMapping(\"/results\") @CrossOrigin public Map<String, JmeterResultData> results() { return jmeterService.getAllResults(); } }","title":"Controller"},{"location":"probe-builder/requirements/","text":"Introduction ProbeBuilder main purpose is to facilitate the process of creation and execution of the jmeter tests for ClusterProbe containers running in the Kubernetes. To clearly identify the goals of this software there are a few functional and non-functional requirements that have been identified and described, and the same as in case of Cluster Probe they also constitute its features. Functional Requirements: Users shall be able to create test specifications using a user-friendly interface, providing details like the number of threads, ramp-up period, endpoint URLs, request methods, and payload data. The system shall generate JMeter test plans based on the test specifications, including: the number of threads request configurations payload values The system shall be able to execute the generated JMeter test plans on the target Kubernetes cluster using the JMeter Java API. After test execution, the system shall collect and store the results, including: response times, success rates errors encountered during the tests The system shall create reports with plots and charts representing the test results, making it easy for users to analyze the performance of their Kubernetes cluster. Non-functional requirements The system shall be able to handle a large number of concurrent users and execute JMeter tests efficiently without significant delays. The user interface should be intuitive and easy to navigate, ensuring that users can create and manage test specifications without extensive training. The system shall be scalable, allowing it to support a growing number of test specifications and users as the demand increases.","title":"Requirements"},{"location":"probe-builder/requirements/#introduction","text":"ProbeBuilder main purpose is to facilitate the process of creation and execution of the jmeter tests for ClusterProbe containers running in the Kubernetes. To clearly identify the goals of this software there are a few functional and non-functional requirements that have been identified and described, and the same as in case of Cluster Probe they also constitute its features.","title":"Introduction"},{"location":"probe-builder/requirements/#functional-requirements","text":"Users shall be able to create test specifications using a user-friendly interface, providing details like the number of threads, ramp-up period, endpoint URLs, request methods, and payload data. The system shall generate JMeter test plans based on the test specifications, including: the number of threads request configurations payload values The system shall be able to execute the generated JMeter test plans on the target Kubernetes cluster using the JMeter Java API. After test execution, the system shall collect and store the results, including: response times, success rates errors encountered during the tests The system shall create reports with plots and charts representing the test results, making it easy for users to analyze the performance of their Kubernetes cluster.","title":"Functional Requirements:"},{"location":"probe-builder/requirements/#non-functional-requirements","text":"The system shall be able to handle a large number of concurrent users and execute JMeter tests efficiently without significant delays. The user interface should be intuitive and easy to navigate, ensuring that users can create and manage test specifications without extensive training. The system shall be scalable, allowing it to support a growing number of test specifications and users as the demand increases.","title":"Non-functional requirements"},{"location":"probe-builder/user-guide/","text":"Introduction In order to start using probe-builder you would most likely first want to set up the cluster-probe . Once you have an instance or a kubernetes cluster running with cluster-probe, you can start using the probe-builder. Probe builder jar file The easiest way to run a probe-builder is to run a jar file, you can download it here: probe-builder 1.0.0 then you can execute a java command to run it: java -jar path/to/the/file/probebuilder.jar now you can enter the browser under the: `localhost:4040' and you should be able to access the probe-builder ui. User interface on the left side you can configure your tests and on the left side after clicking submit you will see the generated payload for your configuration. You can use the generated value for a given test to populate the body form: You can configure the values for your jmeter test as well: Number of threads : Allows the user to input the number of threads for the JMeter test, representing the number of virtual users that will be used to simulate concurrent requests. Ramp-Up Period : Allows the user to specify the ramp-up period for the JMeter test, representing the time (in seconds) taken to reach the maximum number of threads specified. IP : Allows the user to input the IP address of the target server where the JMeter test will be performed. Port : Allows the user to input the port number on which the target server is running. Path : Allows the user to input the API path for the JMeter test. Method : Allows the user to specify the HTTP method (e.g., GET, POST) to be used in the JMeter test. Once we click the submit button, the results table will be populated with our tests: where red background informs the user about the fact that the test has not been finished yet where the green background is applied to the tests have been finished. For each test there is a result file generated with the name of 'test{testId}.csv'. as well as a html file with the plots:","title":"User guide"},{"location":"probe-builder/user-guide/#introduction","text":"In order to start using probe-builder you would most likely first want to set up the cluster-probe . Once you have an instance or a kubernetes cluster running with cluster-probe, you can start using the probe-builder.","title":"Introduction"},{"location":"probe-builder/user-guide/#probe-builder-jar-file","text":"The easiest way to run a probe-builder is to run a jar file, you can download it here: probe-builder 1.0.0 then you can execute a java command to run it: java -jar path/to/the/file/probebuilder.jar now you can enter the browser under the: `localhost:4040' and you should be able to access the probe-builder ui.","title":"Probe builder jar file"},{"location":"probe-builder/user-guide/#user-interface","text":"on the left side you can configure your tests and on the left side after clicking submit you will see the generated payload for your configuration. You can use the generated value for a given test to populate the body form: You can configure the values for your jmeter test as well: Number of threads : Allows the user to input the number of threads for the JMeter test, representing the number of virtual users that will be used to simulate concurrent requests. Ramp-Up Period : Allows the user to specify the ramp-up period for the JMeter test, representing the time (in seconds) taken to reach the maximum number of threads specified. IP : Allows the user to input the IP address of the target server where the JMeter test will be performed. Port : Allows the user to input the port number on which the target server is running. Path : Allows the user to input the API path for the JMeter test. Method : Allows the user to specify the HTTP method (e.g., GET, POST) to be used in the JMeter test. Once we click the submit button, the results table will be populated with our tests: where red background informs the user about the fact that the test has not been finished yet where the green background is applied to the tests have been finished. For each test there is a result file generated with the name of 'test{testId}.csv'. as well as a html file with the plots:","title":"User interface"},{"location":"testing-recluster/setup/","text":"Introduction In this section you will find a guide on how to run tests on reCluster, with ClusterProbe and ProbeBuilder. SSH setup In order to access reCluster you need a configured ssh connection with reLaptop. If you don't have an access, you need to reach Lorenzo Angeli, the project owner. Also remember to configure the VPN connection with the university network (https://icts.unitn.it/en/vpn). Having an ssh connection configured you should be able to connect to the reLaptop: ssh user@10.196.37.164 where user would be your username. now you can ssh to reCluster: ssh recluster Scripts Once you will be logged in into the reCluster you will be able to find a set of scripts in the root directory: pod.sh - this script will keep track of the amount of pods for a specified amount of time, the results will be saved in files node_name_pod_counts.csv. You need to specify the amount of seconds for which this script will be running, it should be equal to the duration of the test. hpa.sh - this script will set a specified target for horizontal pod autoscaling, if no arguments will be passed the default value is 80. By default the maximum amount of pods is set to 1000. no_csa.sh - this script turns off the cluster-autoscaling by constraining the amount of pods to the capacity of the controller. unassing_node.sh - this script unassigned the node from the node pool, the node will be turned off. You need to pass the id of the node as an arugment. downscale.sh - this script will delete horizontal pod autoscaling and downscale pods to 1 Test running Before you run the test make sure that cluster-probe is running by checking: kubectl get pods if the result is different you should create a deployment: kubectl apply -f clusterProbe_test.yaml Now if you want to configure your cluster or track the pods count you can run one of the scripts In order to run tests from the ProbeBuilder you need to configure port forwarding with ssh. Open another window in your terminal and run: ssh -L 9090:localhost:9090 user@10.196.37.164 and now another forward to reCluster: ssh -L 9090:10.43.252.50:8080 root@recluster Now you can run test from your machine on localhost:9090. If you additionally what to be able to monitor your tests you should create similar forwarding on port 3030: relaptop: ssh -L 3030:localhost:3030 user@10.196.37.164 recluster: ssh -L 3030:localhost:8080 recluster Now you should be able to see the table of nodes in the scaling page of ProbeBuilder:","title":"Setup"},{"location":"testing-recluster/setup/#introduction","text":"In this section you will find a guide on how to run tests on reCluster, with ClusterProbe and ProbeBuilder.","title":"Introduction"},{"location":"testing-recluster/setup/#ssh-setup","text":"In order to access reCluster you need a configured ssh connection with reLaptop. If you don't have an access, you need to reach Lorenzo Angeli, the project owner. Also remember to configure the VPN connection with the university network (https://icts.unitn.it/en/vpn). Having an ssh connection configured you should be able to connect to the reLaptop: ssh user@10.196.37.164 where user would be your username. now you can ssh to reCluster: ssh recluster","title":"SSH setup"},{"location":"testing-recluster/setup/#scripts","text":"Once you will be logged in into the reCluster you will be able to find a set of scripts in the root directory: pod.sh - this script will keep track of the amount of pods for a specified amount of time, the results will be saved in files node_name_pod_counts.csv. You need to specify the amount of seconds for which this script will be running, it should be equal to the duration of the test. hpa.sh - this script will set a specified target for horizontal pod autoscaling, if no arguments will be passed the default value is 80. By default the maximum amount of pods is set to 1000. no_csa.sh - this script turns off the cluster-autoscaling by constraining the amount of pods to the capacity of the controller. unassing_node.sh - this script unassigned the node from the node pool, the node will be turned off. You need to pass the id of the node as an arugment. downscale.sh - this script will delete horizontal pod autoscaling and downscale pods to 1","title":"Scripts"},{"location":"testing-recluster/setup/#test-running","text":"Before you run the test make sure that cluster-probe is running by checking: kubectl get pods if the result is different you should create a deployment: kubectl apply -f clusterProbe_test.yaml Now if you want to configure your cluster or track the pods count you can run one of the scripts In order to run tests from the ProbeBuilder you need to configure port forwarding with ssh. Open another window in your terminal and run: ssh -L 9090:localhost:9090 user@10.196.37.164 and now another forward to reCluster: ssh -L 9090:10.43.252.50:8080 root@recluster Now you can run test from your machine on localhost:9090. If you additionally what to be able to monitor your tests you should create similar forwarding on port 3030: relaptop: ssh -L 3030:localhost:3030 user@10.196.37.164 recluster: ssh -L 3030:localhost:8080 recluster Now you should be able to see the table of nodes in the scaling page of ProbeBuilder:","title":"Test running"}]}